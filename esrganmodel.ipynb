{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "esrganmodel.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP++/dmvzCtqnosH6wp6qMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avyay10/Resources-for-SeSiGAN/blob/main/esrganmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a985HbBhblZ"
      },
      "source": [
        "import os\n",
        "import collections\n",
        "import torch\n",
        "\n",
        "class Model(object):\n",
        "    def name(self):\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        pass\n",
        "\n",
        "class FileModel(Model):\n",
        "    def __init__(self, path):\n",
        "        self._model = None\n",
        "        self._path = path\n",
        "\n",
        "    def _get_scale_index(self, state_dict):\n",
        "        # this is more or less guesswork, since I haven't seen any\n",
        "        # non-4x models using the new format in the wild, but it\n",
        "        # should work in theory\n",
        "        max_index = 0\n",
        "\n",
        "        for k in state_dict.keys():\n",
        "            if k.startswith(\"upconv\") and k.endswith(\".weight\"):\n",
        "                max_index = max(max_index, int(k[6:-7]))\n",
        "\n",
        "        return max_index\n",
        "\n",
        "    def _get_legacy_scale_index(self, state_dict):\n",
        "        try:\n",
        "            # get largest model index from keys like \"model.X.weight\"\n",
        "            max_index = max([int(n.split(\".\")[1]) for n in state_dict.keys()])\n",
        "        except:\n",
        "            # invalid model dict format?\n",
        "            raise RuntimeError(\"Unable to determine scale index for model\")\n",
        "\n",
        "        return (max_index - 4) // 3\n",
        "\n",
        "    def _build_legacy_keymap(self, n_upscale):\n",
        "        keymap = collections.OrderedDict()\n",
        "        keymap[\"model.0\"] = \"conv_first\"\n",
        "\n",
        "        for i in range(23):\n",
        "            for j in range(1, 4):\n",
        "                for k in range(1, 6):\n",
        "                    k1 = \"model.1.sub.%d.RDB%d.conv%d.0\" % (i, j, k)\n",
        "                    k2 = \"RRDB_trunk.%d.RDB%d.conv%d\" % (i, j, k)\n",
        "                    keymap[k1] = k2\n",
        "\n",
        "        keymap[\"model.1.sub.23\"] = \"trunk_conv\"\n",
        "\n",
        "        n = 0\n",
        "        for i in range(1, n_upscale + 1):\n",
        "            n += 3\n",
        "            k1 = \"model.%d\" % n\n",
        "            k2 = \"upconv%d\" % i\n",
        "            keymap[k1] = k2\n",
        "\n",
        "        keymap[\"model.%d\" % (n + 2)] = \"HRconv\"\n",
        "        keymap[\"model.%d\" % (n + 4)] = \"conv_last\"\n",
        "\n",
        "        # add \".weigth\" and \".bias\" suffixes to all keys\n",
        "        keymap_final = collections.OrderedDict()\n",
        "\n",
        "        for k1, k2 in keymap.items():\n",
        "            for k_type in (\"weight\", \"bias\"):\n",
        "                k1_f = k1 + \".\" +  k_type\n",
        "                k2_f = k2 + \".\" +  k_type\n",
        "                keymap_final[k1_f] = k2_f\n",
        "\n",
        "        return keymap_final\n",
        "\n",
        "    def name(self):\n",
        "        return os.path.splitext(os.path.basename(self._path))[0]\n",
        "\n",
        "    def _load(self):\n",
        "        state_dict = torch.load(self._path)\n",
        "\n",
        "        # check for legacy model format\n",
        "        if \"model.0.weight\" in state_dict:\n",
        "            # remap dict keys to new format\n",
        "            scale_index = self._get_legacy_scale_index(state_dict)\n",
        "            keymap = self._build_legacy_keymap(scale_index)\n",
        "            state_dict = {keymap[k]: v for k, v in state_dict.items()}\n",
        "        else:\n",
        "            scale_index = self._get_scale_index(state_dict)\n",
        "\n",
        "        return state_dict, scale_index\n",
        "\n",
        "    def load(self):\n",
        "        if self._model is None:\n",
        "            self._model = self._load()\n",
        "        return self._model\n",
        "\n",
        "class WeightedFileListModel(Model):\n",
        "    def __init__(self, weight_map):\n",
        "        self._models = {}\n",
        "        self._total_weigth = 0\n",
        "\n",
        "        names = []\n",
        "        for path, weight in weight_map.items():\n",
        "            model = FileModel(path)\n",
        "            self._models[model] = weight\n",
        "\n",
        "            names.append(model.name())\n",
        "            names.append(str(weight))\n",
        "\n",
        "        self._name = \"_\".join(names)\n",
        "\n",
        "    def name(self):\n",
        "        return self._name\n",
        "\n",
        "    def load(self):\n",
        "        net_interp = collections.OrderedDict()\n",
        "        total_weigth = sum(self._models.values())\n",
        "        scale = 0\n",
        "\n",
        "        for model, weight in self._models.items():\n",
        "            alpha = weight / total_weigth\n",
        "            net, scale = model.load()\n",
        "            for k, v in net.items():\n",
        "                va = alpha * v\n",
        "                if k in net_interp:\n",
        "                    net_interp[k] += va\n",
        "                else:\n",
        "                    net_interp[k] = va\n",
        "\n",
        "        return net_interp, scale"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKAe3gC6hgfj"
      },
      "source": [
        "import functools\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def make_layer(block, n_layers):\n",
        "    layers = []\n",
        "    for _ in range(n_layers):\n",
        "        layers.append(block())\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResidualDenseBlock_5C(nn.Module):\n",
        "    def __init__(self, nf=64, gc=32, bias=True):\n",
        "        super(ResidualDenseBlock_5C, self).__init__()\n",
        "        # gc: growth channel, i.e. intermediate channels\n",
        "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
        "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        # initialization\n",
        "        # mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.lrelu(self.conv1(x))\n",
        "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
        "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
        "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
        "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
        "        return x5 * 0.2 + x\n",
        "\n",
        "\n",
        "class RRDB(nn.Module):\n",
        "    '''Residual in Residual Dense Block'''\n",
        "\n",
        "    def __init__(self, nf, gc=32):\n",
        "        super(RRDB, self).__init__()\n",
        "        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n",
        "        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n",
        "        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.RDB1(x)\n",
        "        out = self.RDB2(out)\n",
        "        out = self.RDB3(out)\n",
        "        return out * 0.2 + x\n",
        "\n",
        "class RRDBNet(nn.Module):\n",
        "    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n",
        "        super(RRDBNet, self).__init__()\n",
        "        RRDB_block_f = functools.partial(RRDB, nf=nf, gc=gc)\n",
        "\n",
        "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
        "        self.RRDB_trunk = make_layer(RRDB_block_f, nb)\n",
        "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        #### upsampling\n",
        "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
        "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
        "\n",
        "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "        self.n_upscale = 0\n",
        "        self.nf = nf\n",
        "\n",
        "    def load_state_dict(self, state_dict, scale, strict=True):\n",
        "        self.n_upscale = scale\n",
        "\n",
        "        # build upconv layers based on model scale\n",
        "        for n in range(1, self.n_upscale + 1):\n",
        "            upconv = nn.Conv2d(self.nf, self.nf, 3, 1, 1, bias=True)\n",
        "            setattr(self, \"upconv%d\" % n, upconv)\n",
        "\n",
        "        return super().load_state_dict(state_dict, strict)\n",
        "\n",
        "    def forward(self, x):\n",
        "        fea = self.conv_first(x)\n",
        "        trunk = self.trunk_conv(self.RRDB_trunk(fea))\n",
        "        fea = fea + trunk\n",
        "\n",
        "        # apply upconv layers\n",
        "        for n in range(1, self.n_upscale + 1):\n",
        "            upconv = getattr(self, \"upconv%d\" % n)\n",
        "            fea = self.lrelu(upconv(F.interpolate(fea, scale_factor=2, mode=\"nearest\")))\n",
        "\n",
        "        out = self.conv_last(self.lrelu(self.HRconv(fea)))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "futUV8vwhrWK"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "#import rrdbnet\n",
        "\n",
        "class Upscaler(object):\n",
        "    def upscale(self, input_image):\n",
        "        # nop\n",
        "        return input_image\n",
        "\n",
        "class RRDBNetUpscaler(Upscaler):\n",
        "    def __init__(self, model, device):\n",
        "        net, scale = model.load()\n",
        "\n",
        "        model_net = rrdbnet.RRDBNet(3, 3, 64, 23)\n",
        "        model_net.load_state_dict(net, scale, strict=True)\n",
        "        model_net.eval()\n",
        "\n",
        "        for _, v in model_net.named_parameters():\n",
        "            v.requires_grad = False\n",
        "\n",
        "        self.model = model_net.to(device)\n",
        "        self.device = device\n",
        "        self.scale_factor = 2 ** scale\n",
        "\n",
        "    def upscale(self, input_image):\n",
        "        input_image = input_image * 1.0 / 255\n",
        "        input_image = np.transpose(input_image[:, :, [2, 1, 0]], (2, 0, 1))\n",
        "        input_image = torch.from_numpy(input_image).float()\n",
        "        input_image = input_image.unsqueeze(0).to(self.device)\n",
        "\n",
        "        output_image = self.model(input_image).data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
        "        output_image = np.transpose(output_image[[2, 1, 0], :, :], (1, 2, 0))\n",
        "        output_image = (output_image * 255.0).round()\n",
        "\n",
        "        return output_image\n",
        "\n",
        "class TiledUpscaler(Upscaler):\n",
        "    def __init__(self, upscaler, tile_size, tile_padding):\n",
        "        self.upscaler = upscaler\n",
        "        self.scale_factor = upscaler.scale_factor\n",
        "        self.tile_size = tile_size\n",
        "        self.tile_padding = tile_padding\n",
        "\n",
        "    def upscale(self, input_image):\n",
        "        scale_factor = self.upscaler.scale_factor\n",
        "        width, height, depth = input_image.shape\n",
        "        output_width = width * scale_factor\n",
        "        output_height = height * scale_factor\n",
        "        output_shape = (output_width, output_height, depth)\n",
        "\n",
        "        # start with black image\n",
        "        output_image = np.zeros(output_shape, np.uint8)\n",
        "\n",
        "        tile_padding = math.ceil(self.tile_size * self.tile_padding)\n",
        "        tile_size = math.ceil(self.tile_size / scale_factor)\n",
        "\n",
        "        tiles_x = math.ceil(width / tile_size)\n",
        "        tiles_y = math.ceil(height / tile_size)\n",
        "\n",
        "        for y in range(tiles_y):\n",
        "            for x in range(tiles_x):\n",
        "                # extract tile from input image\n",
        "                ofs_x = x * tile_size\n",
        "                ofs_y = y * tile_size\n",
        "\n",
        "                # input tile area on total image\n",
        "                input_start_x = ofs_x\n",
        "                input_end_x = min(ofs_x + tile_size, width)\n",
        "\n",
        "                input_start_y = ofs_y\n",
        "                input_end_y = min(ofs_y + tile_size, height)\n",
        "\n",
        "                # input tile area on total image with padding\n",
        "                input_start_x_pad = max(input_start_x - tile_padding, 0)\n",
        "                input_end_x_pad = min(input_end_x + tile_padding, width)\n",
        "\n",
        "                input_start_y_pad = max(input_start_y - tile_padding, 0)\n",
        "                input_end_y_pad = min(input_end_y + tile_padding, height)\n",
        "\n",
        "                # input tile dimensions\n",
        "                input_tile_width = input_end_x - input_start_x\n",
        "                input_tile_height = input_end_y - input_start_y\n",
        "\n",
        "                tile_idx = y * tiles_x + x + 1\n",
        "\n",
        "                print(\"  Tile %d/%d (x=%d y=%d %dx%d)\" % \\\n",
        "                    (tile_idx, tiles_x * tiles_y, x, y, input_tile_width, input_tile_height))\n",
        "\n",
        "                input_tile = input_image[input_start_x_pad:input_end_x_pad, input_start_y_pad:input_end_y_pad]\n",
        "\n",
        "                # upscale tile\n",
        "                output_tile = self.upscaler.upscale(input_tile)\n",
        "\n",
        "                # output tile area on total image\n",
        "                output_start_x = input_start_x * scale_factor\n",
        "                output_end_x = input_end_x * scale_factor\n",
        "\n",
        "                output_start_y = input_start_y * scale_factor\n",
        "                output_end_y = input_end_y * scale_factor\n",
        "\n",
        "                # output tile area without padding\n",
        "                output_start_x_tile = (input_start_x - input_start_x_pad) * scale_factor\n",
        "                output_end_x_tile = output_start_x_tile + input_tile_width * scale_factor\n",
        "\n",
        "                output_start_y_tile = (input_start_y - input_start_y_pad) * scale_factor\n",
        "                output_end_y_tile = output_start_y_tile + input_tile_height * scale_factor\n",
        "\n",
        "                # put tile into output image\n",
        "                output_image[output_start_x:output_end_x, output_start_y:output_end_y] = \\\n",
        "                    output_tile[output_start_x_tile:output_end_x_tile, output_start_y_tile:output_end_y_tile]\n",
        "\n",
        "        return output_image"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99k2CbiEhwcC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}